{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#How To Build a Model: Step By Step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#Pytorch Imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.transforms import ToTensor\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "#Pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x18ba5ed8fd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick Your Universal Hyperparamaters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> torch.cuda.is_available(): True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "epochs = 10\n",
    "input_dim = 512\n",
    "hidden_dim = 100\n",
    "output_dim = 50\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "TORCH_USE_CUDA_DSA=1\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"==>> torch.cuda.is_available(): {torch.cuda.is_available()}\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Your Model Workspace:\n",
    "\n",
    "Your model architecture, and all of it's relevent code, will go in a folder at the location ./NNDL/(Your Model Name Here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Your Dataset/Data Loaders Here\n",
    "\n",
    "Pytorch usually takes a csv file. You need to write a function( For naming convention let's call it *create_torch_datasets()* ) that takes the universal dataset kyle got from parsing and seperates/loads it into data that you want your model to train on\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Custom Dataset Class:\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.data = pd.read_csv(csv_file).dropna()\n",
    "        self.mapping = {}\n",
    "        self.cur_map = 0\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        if  type(self.data.iloc[index, 0]) == float:\n",
    "            t=\"N/A\"\n",
    "            print('N/A Example  In dataset')\n",
    "        else:\n",
    "            t=self.data.iloc[index, 0]\n",
    "        text_o = os.path.join(self.root_dir,\n",
    "            t)\n",
    "        tokens = self.tokenizer(text_o, padding='max_length',truncation=True)['input_ids']\n",
    "        # print(f\"==>> tokens: {tokens}\")\n",
    "        # token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        # print(f\"==>> token_ids: {token_ids}\")\n",
    "        \n",
    "        #input_tensor = torch.tensor(self.tokenizer.convert_tokens_to_ids(tokens)).unsqueeze(0)\n",
    "        category_text = os.path.join(self.root_dir,\n",
    "            self.data.iloc[index, 1])\n",
    "        if category_text in self.mapping:\n",
    "            category = self.mapping[category_text]\n",
    "        else:\n",
    "            self.mapping[category_text] = self.cur_map\n",
    "            print(self.mapping)\n",
    "            category = self.cur_map\n",
    "            self.cur_map+=1\n",
    "        # print(f\"self.mapping:{self.mapping}\")\n",
    "        return torch.tensor(tokens,dtype=torch.float32),category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Hello, how are you doing?\n",
      "Tokens: ['hello', ',', 'how', 'are', 'you', 'doing', '?']\n",
      "Token IDs: tensor([7592, 1010, 2129, 2024, 2017, 2725, 1029])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Input text\n",
    "text = \"Hello, how are you doing?\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Convert tokens to numerical IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Print the results\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", torch.tensor(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is an example of what I'm talking about. The Fashion dataset is pretty easy bc it's function alr exists\n",
    "# but you need to create your own function to make the training data and test_ data datasets\n",
