{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#How To Build a Model: Step By Step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytorch Imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.transforms import ToTensor\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "#Pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haoxu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x1f938afaad0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick Your Universal Hyperparamaters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> torch.cuda.is_available(): True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 10\n",
    "input_dim = 512\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "n_filters=30\n",
    "filter_size=5\n",
    "drop_frac=0.5\n",
    "embed_dim = 200\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"==>> torch.cuda.is_available(): {torch.cuda.is_available()}\")\n",
    "print(device)\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Your Model Workspace:\n",
    "\n",
    "Your model architecture, and all of it's relevent code, will go in a folder at the location ./NNDL/(Your Model Name Here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Your Dataset/Data Loaders Here\n",
    "\n",
    "Pytorch usually takes a csv file. You need to write a function( For naming convention let's call it *create_torch_datasets()* ) that takes the universal dataset kyle got from parsing and seperates/loads it into data that you want your model to train on\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Custom Dataset Class:\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.data = data\n",
    "        self.mapping = {}\n",
    "        self.cur_map = 0\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        if  type(self.data.iloc[index, 0]) == float:\n",
    "            t=\"N/A\"\n",
    "            print('N/A Example  In dataset')\n",
    "        else:\n",
    "            t=self.data.iloc[index, 0]\n",
    "        tokens = self.tokenizer(t, padding='max_length',truncation=True)['input_ids']\n",
    "        # print(f\"==>> tokens: {tokens}\")\n",
    "        # token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        # print(f\"==>> token_ids: {token_ids}\")\n",
    "        \n",
    "        #input_tensor = torch.tensor(self.tokenizer.convert_tokens_to_ids(tokens)).unsqueeze(0)\n",
    "        category_text = os.path.join(self.data.iloc[index, 1])\n",
    "        if category_text in self.mapping:\n",
    "            category = self.mapping[category_text]\n",
    "        else:\n",
    "            self.mapping[category_text] = self.cur_map\n",
    "            print(self.mapping)\n",
    "            category = self.cur_map\n",
    "            self.cur_map+=1\n",
    "        # print(f\"self.mapping:{self.mapping}\")\n",
    "        return torch.tensor(tokens,dtype=torch.long),category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer\n",
    "# from torchtext.data import get_tokenizer\n",
    "# # Load the pre-trained BERT tokenizer\n",
    "# tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# # Input text\n",
    "# text = \"Hello, how are you doing?\"\n",
    "\n",
    "# # Tokenize the text\n",
    "# tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# # Convert tokens to numerical IDs\n",
    "# token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# # Print the results\n",
    "# print(\"Original Text:\", text)\n",
    "# print(\"Tokens:\", tokens)\n",
    "# print(\"Token IDs:\", torch.tensor(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Transformer,CNN,RNN\n",
    "d = pd.read_csv(r'data\\products_noimg_uniform_LDKEbb.csv')\n",
    "text_dataset = TextDataset(data=d,root_dir='/')\n",
    "train,test,trash = torch.utils.data.random_split(text_dataset,[0.8,0.2,0.0])\n",
    "train_dataloader =  DataLoader(train, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=0, drop_last=True)\n",
    "test_dataloader =  DataLoader(test, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=0, drop_last=True)\n",
    "#vocab= build_vocab_from_iterator(train_dataloader,specials=[\"<unk>\"]).to(device)\n",
    "#vocab.set_default_index(vocab[\"<unk>\"])\n",
    "# train_iter= IMDB(split=\"train\")\n",
    "# test_iter = IMDB(split=\"test\")\n",
    "# tokenizer = get_tokenizer(\"basic_english\")\n",
    "# def yield_tokens(data_iter):\n",
    "#     for _, text in data_iter:\n",
    "#         yield tokenizer(text)\n",
    "# train_tokens = []\n",
    "# vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "# vocab.set_default_index(vocab[\"<unk>\"])\n",
    "# test_tokens = []\n",
    "# test_tokens = build_vocab_from_iterator(yield_tokens(test_iter), specials=[\"<unk>\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TextDataset at 0x1f9627b83d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is an example of what I'm talking about. The Fashion dataset is pretty easy bc it's function alr exists\n",
    "# but you need to create your own function to make the training data and test_ data datasets\n",
    "\n",
    "#For Embedding\n",
    "text_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the dataset into data loader, and check the shape, make sure it's how you want it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders.\n",
    "#train_dataloader = DataLoader(text_dataset, batch_size=batch_size,shuffle=True, num_workers=0)\n",
    "#print(train_dataloader[0])\n",
    "# for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "#   print(f\"==>> i_batch: {i_batch}\")\n",
    "#   print(f\"==>> sample_batched: {len(sample_batched)}\")\n",
    "      #print(i_batch, sample_batched['text'][0][0].item(),\n",
    "        #sample_batched['category'])\n",
    "\n",
    "# Display image and label.\n",
    "#train_features, train_labels = next(iter(train_dataloader))\n",
    "# print(train_features)\n",
    "# print(train_labels)\n",
    "# print(f\"Feature batch shape: {train_features.size()}\")\n",
    "# print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "# img = train_features[0].squeeze()\n",
    "# label = train_labels[0]\n",
    "# plt.imshow(img, cmap=\"gray\")\n",
    "# plt.show()\n",
    "# print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Model Architecture\n",
    "\n",
    "1. Create your model architecture in your folder\n",
    "2. Pick your loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "tensor([[-0.0715,  0.0935,  0.0237,  ...,  0.3362,  0.0306,  0.2558],\n",
      "        [ 0.1765,  0.2921, -0.0021,  ..., -0.2077, -0.2319, -0.1081],\n",
      "        [ 0.1229,  0.5804, -0.0696,  ..., -0.0392, -0.1624, -0.0967],\n",
      "        ...,\n",
      "        [ 0.1793, -0.2200,  0.0802,  ..., -0.2685, -0.5130, -0.4049],\n",
      "        [-0.4461,  0.0250,  0.1076,  ..., -0.1996, -0.1753, -0.3110],\n",
      "        [-0.5111, -0.4752,  0.2287,  ..., -0.0057,  0.1647, -0.3907]])\n"
     ]
    }
   ],
   "source": [
    "from NNDL.transformer.architecture import Transformer2\n",
    "from NNDL.RNN.architecture import RNN,MyNetwork\n",
    "from NNDL.Utils.solver import train,test\n",
    "from NNDL.Utils.weight_tracker import ActivationMonitor\n",
    "torch.load(f\".vector_cache/glove.6B.{embed_dim}d.txt.pt\")\n",
    "layer=nn.TransformerEncoderLayer(d_model=512,nhead=16)\n",
    "#model = nn.TransformerEncoder(layer,num_layers=12).to(device)\n",
    "#device=\"cpu\"\n",
    "#model = RNN(input_dim,hidden_dim,output_dim).to(device)\n",
    "model = MyNetwork(input_dim,n_filters,filter_size,drop_frac,output_dim,embed_dim).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-4)\n",
    "activation_monitor = ActivationMonitor(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "This should work to train your model. We may have to make some edits for different optimizations, but we can figure it out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "{'patio, lawn & garden': 0}\n",
      "{'patio, lawn & garden': 0, 'clothing, shoes & jewelry': 1}\n",
      "{'patio, lawn & garden': 0, 'clothing, shoes & jewelry': 1, 'musical instruments': 2}\n",
      "{'patio, lawn & garden': 0, 'clothing, shoes & jewelry': 1, 'musical instruments': 2, 'sports & outdoors': 3}\n",
      "{'patio, lawn & garden': 0, 'clothing, shoes & jewelry': 1, 'musical instruments': 2, 'sports & outdoors': 3, 'electronics': 4}\n",
      "{'patio, lawn & garden': 0, 'clothing, shoes & jewelry': 1, 'musical instruments': 2, 'sports & outdoors': 3, 'electronics': 4, 'arts, crafts & sewing': 5}\n",
      "{'patio, lawn & garden': 0, 'clothing, shoes & jewelry': 1, 'musical instruments': 2, 'sports & outdoors': 3, 'electronics': 4, 'arts, crafts & sewing': 5, 'toys & games': 6}\n",
      "{'patio, lawn & garden': 0, 'clothing, shoes & jewelry': 1, 'musical instruments': 2, 'sports & outdoors': 3, 'electronics': 4, 'arts, crafts & sewing': 5, 'toys & games': 6, 'books': 7}\n",
      "{'patio, lawn & garden': 0, 'clothing, shoes & jewelry': 1, 'musical instruments': 2, 'sports & outdoors': 3, 'electronics': 4, 'arts, crafts & sewing': 5, 'toys & games': 6, 'books': 7, 'grocery & gourmet food': 8}\n",
      "{'patio, lawn & garden': 0, 'clothing, shoes & jewelry': 1, 'musical instruments': 2, 'sports & outdoors': 3, 'electronics': 4, 'arts, crafts & sewing': 5, 'toys & games': 6, 'books': 7, 'grocery & gourmet food': 8, 'health & personal care': 9}\n",
      "loss: 2.302683  [  256/400000]\n",
      "loss: 2.239824  [16640/400000]\n",
      "loss: 2.219281  [33024/400000]\n",
      "loss: 2.238288  [49408/400000]\n",
      "loss: 2.229777  [65792/400000]\n",
      "loss: 2.235021  [82176/400000]\n",
      "loss: 2.221546  [98560/400000]\n",
      "loss: 2.235949  [114944/400000]\n",
      "loss: 2.263555  [131328/400000]\n",
      "loss: 2.183114  [147712/400000]\n",
      "loss: 2.236484  [164096/400000]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    #with torch.autograd.detect_anomaly():\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of layers:\", len(activation_monitor.activations))\n",
    "for i, activation in enumerate(activation_monitor.activations):\n",
    "    print(f\"Layer {i + 1}: {activation.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
