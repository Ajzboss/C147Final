{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#How To Build a Model: Step By Step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytorch Imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.transforms import ToTensor\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "#Pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haoxu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x2308d6fff90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick Your Universal Hyperparamaters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> torch.cuda.is_available(): True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "input_dim = 200\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "n_filters=30\n",
    "filter_size=5\n",
    "drop_frac=0.5\n",
    "embed_dim = 50\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"==>> torch.cuda.is_available(): {torch.cuda.is_available()}\")\n",
    "print(device)\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Your Model Workspace:\n",
    "\n",
    "Your model architecture, and all of it's relevent code, will go in a folder at the location ./NNDL/(Your Model Name Here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Your Dataset/Data Loaders Here\n",
    "\n",
    "Pytorch usually takes a csv file. You need to write a function( For naming convention let's call it *create_torch_datasets()* ) that takes the universal dataset kyle got from parsing and seperates/loads it into data that you want your model to train on\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Custom Dataset Class:\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import GloVe\n",
    "from transformers import BertTokenizer\n",
    "# Load the pre-trained BERT tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Input text\n",
    "text = \"Hello, how are you doing?\"\n",
    "\n",
    "vec = GloVe(name='6B', dim=embed_dim)\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.data = data\n",
    "        self.mapping = {}\n",
    "        self.cur_map = 0\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        if  type(self.data.iloc[index, 0]) == float:\n",
    "            t=\"N/A\"\n",
    "            print('N/A Example  In dataset')\n",
    "        else:\n",
    "            t=self.data.iloc[index, 0]\n",
    "        #print(t)\n",
    "        tokens = tokenizer.tokenize(t)\n",
    "        #print(tokens)\n",
    "        token_length=input_dim\n",
    "        tokens=tokens+[\"\"] * (token_length-len(tokens))  if len(tokens)<token_length else tokens[:token_length]\n",
    "        #print(tokens)\n",
    "        tokens_emb = [vec.stoi.get(token, 0) for token in tokens]\n",
    "        #print(tokens_emb)\n",
    "        #print(tokens_emb.shape)\n",
    "        # print(f\"==>> tokens: {tokens}\")\n",
    "        # token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        # print(f\"==>> token_ids: {token_ids}\")\n",
    "        \n",
    "        #input_tensor = torch.tensor(self.tokenizer.convert_tokens_to_ids(tokens)).unsqueeze(0)\n",
    "        category_text = os.path.join(self.data.iloc[index, 1])\n",
    "        if category_text in self.mapping:\n",
    "            category = self.mapping[category_text]\n",
    "        else:\n",
    "            self.mapping[category_text] = self.cur_map\n",
    "            #print(self.mapping)\n",
    "            category = self.cur_map\n",
    "            self.cur_map+=1\n",
    "        # print(f\"self.mapping:{self.mapping}\")\n",
    "        return torch.tensor(tokens_emb),category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13075,     1,   197,    32,    81,   914,   188])\n",
      "Original Text: Hello, how are you doing?\n",
      "Tokens: ['hello', ',', 'how', 'are', 'you', 'doing', '?']\n",
      "Token IDs: tensor([7592, 1010, 2129, 2024, 2017, 2725, 1029])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Input text\n",
    "text = \"Hello, how are you doing?\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens_emb = [vec.stoi[token] for token in tokens]\n",
    "print(torch.tensor(tokens_emb))\n",
    "# Convert tokens to numerical IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Print the results\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", torch.tensor(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Transformer,CNN,RNN\n",
    "d = pd.read_csv(r'data\\products_noimg_uniform_LDKEbb.csv')\n",
    "\n",
    "text_dataset = TextDataset(data=d,root_dir='/')\n",
    "\n",
    "train,test,trash = torch.utils.data.random_split(text_dataset,[0.8,0.2,0.0])\n",
    "train_dataloader =  DataLoader(train, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=0, drop_last=True)\n",
    "test_dataloader =  DataLoader(test, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=0, drop_last=True)\n",
    "#vocab= build_vocab_from_iterator(train_dataloader,specials=[\"<unk>\"]).to(device)\n",
    "#vocab.set_default_index(vocab[\"<unk>\"])\n",
    "# train_iter= IMDB(split=\"train\")\n",
    "# test_iter = IMDB(split=\"test\")\n",
    "# tokenizer = get_tokenizer(\"basic_english\")\n",
    "# def yield_tokens(data_iter):\n",
    "#     for _, text in data_iter:\n",
    "#         yield tokenizer(text)\n",
    "# train_tokens = []\n",
    "# vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "# vocab.set_default_index(vocab[\"<unk>\"])\n",
    "# test_tokens = []\n",
    "# test_tokens = build_vocab_from_iterator(yield_tokens(test_iter), specials=[\"<unk>\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TextDataset at 0x230b8d24350>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is an example of what I'm talking about. The Fashion dataset is pretty easy bc it's function alr exists\n",
    "# but you need to create your own function to make the training data and test_ data datasets\n",
    "\n",
    "#For Embedding\n",
    "text_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the dataset into data loader, and check the shape, make sure it's how you want it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders.\n",
    "#train_dataloader = DataLoader(text_dataset, batch_size=batch_size,shuffle=True, num_workers=0)\n",
    "#print(train_dataloader[0])\n",
    "# for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "#   print(f\"==>> i_batch: {i_batch}\")\n",
    "#   print(f\"==>> sample_batched: {len(sample_batched)}\")\n",
    "      #print(i_batch, sample_batched['text'][0][0].item(),\n",
    "        #sample_batched['category'])\n",
    "\n",
    "# Display image and label.\n",
    "#train_features, train_labels = next(iter(train_dataloader))\n",
    "# print(train_features)\n",
    "# print(train_labels)\n",
    "# print(f\"Feature batch shape: {train_features.size()}\")\n",
    "# print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "# img = train_features[0].squeeze()\n",
    "# label = train_labels[0]\n",
    "# plt.imshow(img, cmap=\"gray\")\n",
    "# plt.show()\n",
    "# print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Model Architecture\n",
    "\n",
    "1. Create your model architecture in your folder\n",
    "2. Pick your loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from NNDL.transformer.architecture import Transformer2\n",
    "from NNDL.RNN.architecture import RNN,MyNetwork\n",
    "from NNDL.Utils.solver import train,test\n",
    "from NNDL.Utils.weight_tracker import ActivationMonitor\n",
    "layer=nn.TransformerEncoderLayer(d_model=512,nhead=16)\n",
    "#model = nn.TransformerEncoder(layer,num_layers=12).to(device)\n",
    "#device=\"cpu\"\n",
    "#model = RNN(input_dim,hidden_dim,output_dim).to(device)\n",
    "model = MyNetwork(embed_dim,n_filters,filter_size,drop_frac,output_dim,embed_dim).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "activation_monitor = ActivationMonitor(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "This should work to train your model. We may have to make some edits for different optimizations, but we can figure it out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.302938  [   32/400000]\n",
      "loss: 2.282594  [ 2080/400000]\n",
      "loss: 2.250029  [ 4128/400000]\n",
      "loss: 2.120448  [ 6176/400000]\n",
      "loss: 2.024965  [ 8224/400000]\n",
      "loss: 1.885405  [10272/400000]\n",
      "loss: 2.057969  [12320/400000]\n",
      "loss: 2.050854  [14368/400000]\n",
      "loss: 1.776759  [16416/400000]\n",
      "loss: 2.011277  [18464/400000]\n",
      "loss: 1.891086  [20512/400000]\n",
      "loss: 2.117066  [22560/400000]\n",
      "loss: 2.210536  [24608/400000]\n",
      "loss: 2.230257  [26656/400000]\n",
      "loss: 2.026537  [28704/400000]\n",
      "loss: 1.867089  [30752/400000]\n",
      "loss: 1.936253  [32800/400000]\n",
      "loss: 1.941193  [34848/400000]\n",
      "loss: 2.117586  [36896/400000]\n",
      "loss: 2.062722  [38944/400000]\n",
      "loss: 1.991316  [40992/400000]\n",
      "loss: 1.900179  [43040/400000]\n",
      "loss: 2.021216  [45088/400000]\n",
      "loss: 1.954666  [47136/400000]\n",
      "loss: 2.049567  [49184/400000]\n",
      "loss: 1.869415  [51232/400000]\n",
      "loss: 1.862399  [53280/400000]\n",
      "loss: 2.039864  [55328/400000]\n",
      "loss: 2.304670  [57376/400000]\n",
      "loss: 2.051107  [59424/400000]\n",
      "loss: 2.120418  [61472/400000]\n",
      "loss: 1.992298  [63520/400000]\n",
      "loss: 1.891220  [65568/400000]\n",
      "loss: 2.022423  [67616/400000]\n",
      "loss: 1.949823  [69664/400000]\n",
      "loss: 2.088136  [71712/400000]\n",
      "loss: 2.117201  [73760/400000]\n",
      "loss: 2.054594  [75808/400000]\n",
      "loss: 1.991737  [77856/400000]\n",
      "loss: 1.961150  [79904/400000]\n",
      "loss: 1.961150  [81952/400000]\n",
      "loss: 1.977032  [84000/400000]\n",
      "loss: 1.949140  [86048/400000]\n",
      "loss: 1.954082  [88096/400000]\n",
      "loss: 2.048488  [90144/400000]\n",
      "loss: 1.992372  [92192/400000]\n",
      "loss: 1.968113  [94240/400000]\n",
      "loss: 2.148500  [96288/400000]\n",
      "loss: 1.992020  [98336/400000]\n",
      "loss: 2.022547  [100384/400000]\n",
      "loss: 1.961819  [102432/400000]\n",
      "loss: 2.037285  [104480/400000]\n",
      "loss: 1.961127  [106528/400000]\n",
      "loss: 2.117088  [108576/400000]\n",
      "loss: 2.053066  [110624/400000]\n",
      "loss: 1.991087  [112672/400000]\n",
      "loss: 2.147323  [114720/400000]\n",
      "loss: 1.846860  [116768/400000]\n",
      "loss: 1.923865  [118816/400000]\n",
      "loss: 2.134533  [120864/400000]\n",
      "loss: 2.117284  [122912/400000]\n",
      "loss: 1.992400  [124960/400000]\n",
      "loss: 2.066289  [127008/400000]\n",
      "loss: 1.932122  [129056/400000]\n",
      "loss: 1.961130  [131104/400000]\n",
      "loss: 1.871159  [133152/400000]\n",
      "loss: 2.054901  [135200/400000]\n",
      "loss: 1.961150  [137248/400000]\n",
      "loss: 2.086130  [139296/400000]\n",
      "loss: 2.038804  [141344/400000]\n",
      "loss: 1.992257  [143392/400000]\n",
      "loss: 1.898847  [145440/400000]\n",
      "loss: 2.085048  [147488/400000]\n",
      "loss: 1.961104  [149536/400000]\n",
      "loss: 1.898092  [151584/400000]\n",
      "loss: 2.054553  [153632/400000]\n",
      "loss: 2.211151  [155680/400000]\n",
      "loss: 2.117401  [157728/400000]\n",
      "loss: 2.003005  [159776/400000]\n",
      "loss: 2.086150  [161824/400000]\n",
      "loss: 1.898751  [163872/400000]\n",
      "loss: 2.054757  [165920/400000]\n",
      "loss: 1.992344  [167968/400000]\n",
      "loss: 2.148632  [170016/400000]\n",
      "loss: 1.961151  [172064/400000]\n",
      "loss: 1.992401  [174112/400000]\n",
      "loss: 2.095286  [176160/400000]\n",
      "loss: 2.117204  [178208/400000]\n",
      "loss: 2.086151  [180256/400000]\n",
      "loss: 2.054564  [182304/400000]\n",
      "loss: 1.867162  [184352/400000]\n",
      "loss: 1.992010  [186400/400000]\n",
      "loss: 2.117401  [188448/400000]\n",
      "loss: 2.148651  [190496/400000]\n",
      "loss: 2.117151  [192544/400000]\n",
      "loss: 2.086151  [194592/400000]\n",
      "loss: 2.085362  [196640/400000]\n",
      "loss: 2.086090  [198688/400000]\n",
      "loss: 2.148651  [200736/400000]\n",
      "loss: 2.054901  [202784/400000]\n",
      "loss: 1.961150  [204832/400000]\n",
      "loss: 2.023651  [206880/400000]\n",
      "loss: 1.929901  [208928/400000]\n",
      "loss: 1.992401  [210976/400000]\n",
      "loss: 1.961151  [213024/400000]\n",
      "loss: 1.992399  [215072/400000]\n",
      "loss: 1.961527  [217120/400000]\n",
      "loss: 1.804901  [219168/400000]\n",
      "loss: 2.367401  [221216/400000]\n",
      "loss: 2.304900  [223264/400000]\n",
      "loss: 1.961151  [225312/400000]\n",
      "loss: 2.023651  [227360/400000]\n",
      "loss: 2.054901  [229408/400000]\n",
      "loss: 2.117401  [231456/400000]\n",
      "loss: 2.023653  [233504/400000]\n",
      "loss: 2.148651  [235552/400000]\n",
      "loss: 1.992401  [237600/400000]\n",
      "loss: 2.023651  [239648/400000]\n",
      "loss: 2.086151  [241696/400000]\n",
      "loss: 2.179599  [243744/400000]\n",
      "loss: 2.054901  [245792/400000]\n",
      "loss: 2.179129  [247840/400000]\n",
      "loss: 2.117400  [249888/400000]\n",
      "loss: 2.179899  [251936/400000]\n",
      "loss: 2.117401  [253984/400000]\n",
      "loss: 1.961151  [256032/400000]\n",
      "loss: 2.054901  [258080/400000]\n",
      "loss: 2.054901  [260128/400000]\n",
      "loss: 2.054890  [262176/400000]\n",
      "loss: 1.773651  [264224/400000]\n",
      "loss: 2.054901  [266272/400000]\n",
      "loss: 2.117401  [268320/400000]\n",
      "loss: 2.117401  [270368/400000]\n",
      "loss: 1.773667  [272416/400000]\n",
      "loss: 2.027558  [274464/400000]\n",
      "loss: 2.023649  [276512/400000]\n",
      "loss: 2.023651  [278560/400000]\n",
      "loss: 1.867401  [280608/400000]\n",
      "loss: 1.773650  [282656/400000]\n",
      "loss: 2.117400  [284704/400000]\n",
      "loss: 1.961150  [286752/400000]\n",
      "loss: 1.992401  [288800/400000]\n",
      "loss: 1.836151  [290848/400000]\n",
      "loss: 2.148650  [292896/400000]\n",
      "loss: 2.054901  [294944/400000]\n",
      "loss: 1.992400  [296992/400000]\n",
      "loss: 1.961139  [299040/400000]\n",
      "loss: 1.898651  [301088/400000]\n",
      "loss: 1.992401  [303136/400000]\n",
      "loss: 1.904345  [305184/400000]\n",
      "loss: 2.117401  [307232/400000]\n",
      "loss: 2.086151  [309280/400000]\n",
      "loss: 1.961150  [311328/400000]\n",
      "loss: 2.084637  [313376/400000]\n",
      "loss: 2.148183  [315424/400000]\n",
      "loss: 2.023650  [317472/400000]\n",
      "loss: 2.023651  [319520/400000]\n",
      "loss: 2.086151  [321568/400000]\n",
      "loss: 1.992401  [323616/400000]\n",
      "loss: 2.053900  [325664/400000]\n",
      "loss: 2.179499  [327712/400000]\n",
      "loss: 2.086151  [329760/400000]\n",
      "loss: 2.054900  [331808/400000]\n",
      "loss: 2.023650  [333856/400000]\n",
      "loss: 2.211151  [335904/400000]\n",
      "loss: 1.929883  [337952/400000]\n",
      "loss: 2.021336  [340000/400000]\n",
      "loss: 2.211147  [342048/400000]\n",
      "loss: 2.165033  [344096/400000]\n",
      "loss: 2.023650  [346144/400000]\n",
      "loss: 2.211151  [348192/400000]\n",
      "loss: 1.991915  [350240/400000]\n",
      "loss: 2.023650  [352288/400000]\n",
      "loss: 2.148650  [354336/400000]\n",
      "loss: 2.086093  [356384/400000]\n",
      "loss: 1.929900  [358432/400000]\n",
      "loss: 1.961150  [360480/400000]\n",
      "loss: 1.773651  [362528/400000]\n",
      "loss: 1.992401  [364576/400000]\n",
      "loss: 2.086151  [366624/400000]\n",
      "loss: 2.117401  [368672/400000]\n",
      "loss: 1.930017  [370720/400000]\n",
      "loss: 1.960171  [372768/400000]\n",
      "loss: 2.021836  [374816/400000]\n",
      "loss: 2.086154  [376864/400000]\n",
      "loss: 2.023650  [378912/400000]\n",
      "loss: 2.148651  [380960/400000]\n",
      "loss: 2.211151  [383008/400000]\n",
      "loss: 2.054901  [385056/400000]\n",
      "loss: 1.929901  [387104/400000]\n",
      "loss: 2.148651  [389152/400000]\n",
      "loss: 1.992401  [391200/400000]\n",
      "loss: 2.086151  [393248/400000]\n",
      "loss: 2.023651  [395296/400000]\n",
      "loss: 2.023651  [397344/400000]\n",
      "loss: 2.179901  [399392/400000]\n",
      "Test Error: \n",
      " Accuracy: 44.5%, Avg loss: 2.015986 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.054901  [   32/400000]\n",
      "loss: 1.961151  [ 2080/400000]\n",
      "loss: 2.117401  [ 4128/400000]\n",
      "loss: 2.054901  [ 6176/400000]\n",
      "loss: 1.711151  [ 8224/400000]\n",
      "loss: 2.086151  [10272/400000]\n",
      "loss: 2.086151  [12320/400000]\n",
      "loss: 1.867401  [14368/400000]\n",
      "loss: 1.992401  [16416/400000]\n",
      "loss: 1.961151  [18464/400000]\n",
      "loss: 2.117401  [20512/400000]\n",
      "loss: 1.961151  [22560/400000]\n",
      "loss: 2.211151  [24608/400000]\n",
      "loss: 2.086151  [26656/400000]\n",
      "loss: 2.086151  [28704/400000]\n",
      "loss: 2.242401  [30752/400000]\n",
      "loss: 2.086151  [32800/400000]\n",
      "loss: 2.179659  [34848/400000]\n",
      "loss: 1.992401  [36896/400000]\n",
      "loss: 2.366461  [38944/400000]\n",
      "loss: 1.961151  [40992/400000]\n",
      "loss: 2.211151  [43040/400000]\n",
      "loss: 2.023650  [45088/400000]\n",
      "loss: 2.023651  [47136/400000]\n",
      "loss: 2.086151  [49184/400000]\n",
      "loss: 2.273651  [51232/400000]\n",
      "loss: 2.304901  [53280/400000]\n",
      "loss: 2.273651  [55328/400000]\n",
      "loss: 2.023650  [57376/400000]\n",
      "loss: 2.273651  [59424/400000]\n",
      "loss: 2.273651  [61472/400000]\n",
      "loss: 2.117401  [63520/400000]\n",
      "loss: 2.148651  [65568/400000]\n",
      "loss: 2.148651  [67616/400000]\n",
      "loss: 2.273651  [69664/400000]\n",
      "loss: 2.273651  [71712/400000]\n",
      "loss: 2.148651  [73760/400000]\n",
      "loss: 2.054901  [75808/400000]\n",
      "loss: 2.148651  [77856/400000]\n",
      "loss: 2.148651  [79904/400000]\n",
      "loss: 2.148651  [81952/400000]\n",
      "loss: 2.211151  [84000/400000]\n",
      "loss: 2.117401  [86048/400000]\n",
      "loss: 2.086151  [88096/400000]\n",
      "loss: 2.023651  [90144/400000]\n",
      "loss: 1.961151  [92192/400000]\n",
      "loss: 1.867401  [94240/400000]\n",
      "loss: 2.179901  [96288/400000]\n",
      "loss: 1.961151  [98336/400000]\n",
      "loss: 1.898651  [100384/400000]\n",
      "loss: 1.961151  [102432/400000]\n",
      "loss: 2.179900  [104480/400000]\n",
      "loss: 2.023651  [106528/400000]\n",
      "loss: 2.086151  [108576/400000]\n",
      "loss: 2.086151  [110624/400000]\n",
      "loss: 2.086151  [112672/400000]\n",
      "loss: 2.117401  [114720/400000]\n",
      "loss: 2.242401  [116768/400000]\n",
      "loss: 1.961151  [118816/400000]\n",
      "loss: 2.086151  [120864/400000]\n",
      "loss: 2.023651  [122912/400000]\n",
      "loss: 1.961151  [124960/400000]\n",
      "loss: 1.898651  [127008/400000]\n",
      "loss: 2.054901  [129056/400000]\n",
      "loss: 2.023651  [131104/400000]\n",
      "loss: 2.211150  [133152/400000]\n",
      "loss: 2.117401  [135200/400000]\n",
      "loss: 2.211150  [137248/400000]\n",
      "loss: 2.117401  [139296/400000]\n",
      "loss: 2.242401  [141344/400000]\n",
      "loss: 2.054900  [143392/400000]\n",
      "loss: 2.023650  [145440/400000]\n",
      "loss: 1.992401  [147488/400000]\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    #with torch.autograd.detect_anomaly():\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of layers:\", len(activation_monitor.activations))\n",
    "for i, activation in enumerate(activation_monitor.activations):\n",
    "    print(f\"Layer {i + 1}: {activation.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
